{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Steps are:\n",
    "\n",
    "- select a reconstruction time\n",
    "- the code determines which paleogeography stage this falls within, gets the start and end times\n",
    "- load the relevant precomputed multipoint files, and in the process assign an integer to the different types for use in interpolation steps (e.g. set land to be 1, shallow marine to be 2, etc)\n",
    "\n",
    "- for land and marine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pygplates\n",
    "import glob, re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import xarray as xr\n",
    "\n",
    "import polygon_processing as pp\n",
    "import paleogeography as pg\n",
    "import paleogeography_tweening as pgt\n",
    "\n",
    "from proximity_query import *\n",
    "from create_gpml import create_gpml_regular_long_lat_mesh\n",
    "import points_in_polygons\n",
    "from sphere_tools import sampleOnSphere\n",
    "import points_spatial_tree\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "reconstruction_time = 70.\n",
    "\n",
    "reconstruction_basedir = './Paleogeography_Matthews2016_410-2Ma_Shapefiles/'\n",
    "tween_basedir = './tween_feature_collections/'\n",
    "\n",
    "output_dir = './paleotopo_grids/'\n",
    "\n",
    "area_threshold = 0.001\n",
    "\n",
    "sampling = 0.5\n",
    "\n",
    "#rotation_model = pygplates.RotationModel(['%s/Global_EB_250-0Ma_GK07_Matthews++.rot' % reconstruction_basedir,\n",
    "#                                          '%s/Global_EB_410-250Ma_GK07_Matthews++.rot' % reconstruction_basedir])\n",
    "rotation_model = pygplates.RotationModel('%s/Global_EarthByte_230-0Ma_GK07_AREPS.rot' % reconstruction_basedir)\n",
    "\n",
    "\n",
    "COBterrane_file = '%s/Global_EarthByte_GeeK07_COB_Terranes_Matthews_etal.gpml' % reconstruction_basedir\n",
    "\n",
    "agegrid_file_template = '/Users/Simon/Data/AgeGrids/Agegrids_30m_20151002_2015_v1_r756/agegrid_30m_%d.grd'\n",
    "\n",
    "\n",
    "depth_for_unknown_ocean = -1000\n",
    "max_mountain_elevation = 3000.\n",
    "\n",
    "\n",
    "####################################################\n",
    "\n",
    "# make a sorted list of the (midpoint) times for paleogeography polygons\n",
    "tmp = glob.glob('%s/*/' % reconstruction_basedir)\n",
    "\n",
    "time_list = []\n",
    "for tm in tmp:\n",
    "    time_list.append(float(re.findall(r'\\d+Ma+',tm)[1][:-2]))\n",
    "\n",
    "time_list.sort()\n",
    "\n",
    "time_list = np.array(time_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def write_xyz_file(output_filename, output_data):\n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        for output_line in output_data:\n",
    "            output_file.write(' '.join(str(item) for item in output_line) + '\\n')\n",
    "\n",
    "\n",
    "# define a function that loads paleogeography multipoints at a specified time\n",
    "# NOTE this time can be anything, not a time where the multipoints fit nicely together,\n",
    "# hence the gaps and overlaps will be present\n",
    "def add_reconstructed_points_to_xyz(points_file,rotation_model,reconstruction_time,zval):\n",
    "    \n",
    "    reconstructed_points = []\n",
    "    pygplates.reconstruct(points_file,rotation_model,reconstructed_points,reconstruction_time)\n",
    "    \n",
    "    point_array = []\n",
    "    for reconstructed_point in reconstructed_points:\n",
    "        point_array.append(reconstructed_point.get_reconstructed_geometry().to_lat_lon_array())\n",
    "        \n",
    "    print points_file\n",
    "    xy_array = np.vstack(point_array)\n",
    "    xyz_array = np.hstack((xy_array,zval*np.ones((xy_array.shape[0],1))))\n",
    "    \n",
    "    return xyz_array\n",
    "\n",
    "\n",
    "def get_distance_to_mountain_edge(point_array,reconstruction_basedir,time):\n",
    "    \n",
    "    distance_threshold_radians=None\n",
    "    env_list = ['m']\n",
    "\n",
    "    pg_dir = '%s/PresentDay_Paleogeog_Matthews2016_%dMa/' % (reconstruction_basedir,time)\n",
    "\n",
    "    pg_features = pg.load_paleogeography(pg_dir,env_list)\n",
    "    cf = pp.merge_polygons(pg_features,rotation_model,time=time,sampling=0.25)\n",
    "    sieve_polygons_t1 = pp.polygon_area_threshold(cf,area_threshold)\n",
    "\n",
    "    polygons_as_list = []\n",
    "    for feature in sieve_polygons_t1:\n",
    "        polygons_as_list.append(feature.get_geometry())\n",
    "        \n",
    "    res1 = find_closest_geometries_to_points([pygplates.PointOnSphere(point) for point in zip(point_array[:,0],point_array[:,1])],\n",
    "                                             polygons_as_list,\n",
    "                                             distance_threshold_radians = distance_threshold_radians)\n",
    "    \n",
    "    distance_to_polygon_boundary = np.degrees(np.array(zip(*res1)[0]))\n",
    "\n",
    "    # Make a copy of list of distances.\n",
    "    distance_to_polygon = list(distance_to_polygon_boundary)\n",
    "\n",
    "    # Set distance to zero for any points inside a polygon (leave other points unchanged).\n",
    "    res2 = points_in_polygons.find_polygons([pygplates.PointOnSphere(point) for point in zip(point_array[:,0],point_array[:,1])],\n",
    "                                            polygons_as_list)\n",
    "\n",
    "    for point_index, rpolygon in enumerate(res2):\n",
    "        # If not inside any polygons then result will be None.\n",
    "        if rpolygon is None:\n",
    "            distance_to_polygon[point_index] = 0.0\n",
    "            \n",
    "    return distance_to_polygon\n",
    "\n",
    "\n",
    "# This cell uses COB Terranes to make a masking polygon\n",
    "# (which is called 'seive_polygons')\n",
    "def get_merged_cob_terrane_polygons(COBterrane_file,reconstruction_time,sampling):\n",
    "\n",
    "    polygon_features = pygplates.FeatureCollection(COBterrane_file)\n",
    "\n",
    "    cobter = pp.force_polygon_geometries(polygon_features)\n",
    "\n",
    "    cf = pp.merge_polygons(cobter,rotation_model,time=reconstruction_time,sampling=sampling)\n",
    "    sieve_polygons = pp.polygon_area_threshold(cf,area_threshold)\n",
    "\n",
    "    return sieve_polygons\n",
    "\n",
    "\n",
    "\n",
    "# use merged seive_polygons to get a regular lat-long multipoint that will contain points\n",
    "# only within the COB Terranes (ie not within the 'deep ocean')\n",
    "def get_land_sea_multipoints(sieve_polygons,sampling):\n",
    "\n",
    "    multipoints = create_gpml_regular_long_lat_mesh(sampling)\n",
    "    grid_dims = (int(180/sampling)+1,int(360/sampling)+1)\n",
    "\n",
    "    for multipoint in multipoints:\n",
    "        for mp in multipoint.get_all_geometries():\n",
    "            points = mp.to_lat_lon_point_list()\n",
    "\n",
    "    #reconstructed_polygons = []\n",
    "    #pygplates.reconstruct(cobter,rotation_model,reconstructed_polygons,reconstruction_time)\n",
    "\n",
    "    rpolygons = []\n",
    "    for polygon in sieve_polygons:\n",
    "        if polygon.get_geometry():\n",
    "            rpolygons.append(polygon.get_geometry())\n",
    "\n",
    "    polygons_containing_points = points_in_polygons.find_polygons(points, rpolygons, subdivision_depth=2)\n",
    "\n",
    "    lat = []\n",
    "    lon = []\n",
    "    zval = []\n",
    "\n",
    "    lat_deep = []\n",
    "    lon_deep = []\n",
    "    zval_deep = []\n",
    "\n",
    "    for pcp,point in zip(polygons_containing_points,points):\n",
    "        if pcp is not None:\n",
    "            lat.append(point.get_latitude())\n",
    "            lon.append(point.get_longitude())\n",
    "        else:\n",
    "            lat_deep.append(point.get_latitude())\n",
    "            lon_deep.append(point.get_longitude())\n",
    "            zval_deep.append(depth_for_unknown_ocean)\n",
    "            \n",
    "    plt.figure(figsize=(25,11))      \n",
    "    plt.plot(lon,lat,'.')\n",
    "    \n",
    "    plt.figure(figsize=(25,11))      \n",
    "    plt.plot(lon_deep,lat_deep,'.')\n",
    "    \n",
    "    plt.figure(figsize=(25,11))  \n",
    "    for polygon in rpolygons:\n",
    "        plt.plot(polygon.to_lat_lon_array()[:,1],\n",
    "                 polygon.to_lat_lon_array()[:,0])\n",
    "        \n",
    "            \n",
    "    return lat,lon,zval,lat_deep,lon_deep,zval_deep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Time 14.00Ma\n",
      "\n",
      "Selected Time is in the stage 14.00Ma to 22.00Ma\n",
      "./tween_feature_collections//tweentest_land_14.00Ma_22.00Ma.gpmlz\n",
      "./tween_feature_collections//tweentest_ocean_14.00Ma_22.00Ma.gpmlz\n",
      "Temporary hack for valid time\n",
      "./tween_feature_collections//mountain_regression_14.00Ma_22.00Ma.gpmlz\n",
      "./tween_feature_collections//mountain_stable_14.00Ma_22.00Ma.gpmlz\n",
      "['./Paleogeography_Matthews2016_410-2Ma_Shapefiles//PresentDay_Paleogeog_Matthews2016_14Ma/m_fig62_20_11_PresentDay_Paleogeog_Matthews2016_14.00Ma.shp']\n",
      "Working on Time 15.00Ma\n",
      "\n",
      "Selected Time is in the stage 14.00Ma to 22.00Ma\n",
      "./tween_feature_collections//tweentest_land_14.00Ma_22.00Ma.gpmlz\n",
      "./tween_feature_collections//tweentest_ocean_14.00Ma_22.00Ma.gpmlz\n",
      "./tween_feature_collections//mountain_transgression_14.00Ma_22.00Ma.gpmlz\n",
      "./tween_feature_collections//mountain_regression_14.00Ma_22.00Ma.gpmlz\n",
      "./tween_feature_collections//mountain_stable_14.00Ma_22.00Ma.gpmlz\n",
      "./tween_feature_collections//mountain_transgression_14.00Ma_22.00Ma.gpmlz\n",
      "./tween_feature_collections//mountain_regression_14.00Ma_22.00Ma.gpmlz\n",
      "./tween_feature_collections//mountain_stable_14.00Ma_22.00Ma.gpmlz\n",
      "./tween_feature_collections//mountain_transgression_14.00Ma_22.00Ma.gpmlz\n",
      "./tween_feature_collections//mountain_regression_14.00Ma_22.00Ma.gpmlz\n",
      "./tween_feature_collections//mountain_stable_14.00Ma_22.00Ma.gpmlz\n",
      "['./Paleogeography_Matthews2016_410-2Ma_Shapefiles//PresentDay_Paleogeog_Matthews2016_14Ma/m_fig62_20_11_PresentDay_Paleogeog_Matthews2016_14.00Ma.shp']\n"
     ]
    }
   ],
   "source": [
    "for reconstruction_time in np.arange(14,231,1):\n",
    "\n",
    "    print 'Working on Time %0.2fMa\\n' % reconstruction_time \n",
    "        \n",
    "    # find times that bracket the selected exact time in the paleogeography source files\n",
    "    time_stage_max = time_list[np.where(time_list>reconstruction_time)[0][0]]\n",
    "    time_stage_min = time_list[np.where(time_list<=reconstruction_time)[0][-1]]\n",
    "\n",
    "    # Note the logica for selecting the times:\n",
    "    # The main issue is that each set of paleogeography polygons has a defined 'midpoint' time\n",
    "    # --> if the reconstruction time is between these, the choice of t1 and t2 is obvious\n",
    "    # --> if the reconstruction time matches one of these times, then  \n",
    "    \n",
    "    print 'Selected Time is in the stage %0.2fMa to %0.2fMa' % (time_stage_min,time_stage_max)\n",
    "\n",
    "\n",
    "    land_points_file = '%s/tweentest_land_%0.2fMa_%0.2fMa.gpmlz' % (tween_basedir,time_stage_min,time_stage_max)\n",
    "    marine_points_file = '%s/tweentest_ocean_%0.2fMa_%0.2fMa.gpmlz' % (tween_basedir,time_stage_min,time_stage_max)\n",
    "    mountains_going_up_file = '%s/mountain_transgression_%0.2fMa_%0.2fMa.gpmlz' % (tween_basedir,time_stage_min,time_stage_max)\n",
    "    mountains_going_down_file = '%s/mountain_regression_%0.2fMa_%0.2fMa.gpmlz' % (tween_basedir,time_stage_min,time_stage_max)\n",
    "    mountains_stable_file = '%s/mountain_stable_%0.2fMa_%0.2fMa.gpmlz' % (tween_basedir,time_stage_min,time_stage_max)\n",
    "\n",
    "    \n",
    "    land_point_array = add_reconstructed_points_to_xyz(land_points_file,rotation_model,reconstruction_time,200)\n",
    "    marine_point_array = add_reconstructed_points_to_xyz(marine_points_file,rotation_model,reconstruction_time,-200)\n",
    "\n",
    "    pg_point_array = np.vstack((land_point_array,marine_point_array))\n",
    "\n",
    "\n",
    "    sieve_polygons = get_merged_cob_terrane_polygons(COBterrane_file,reconstruction_time,sampling)\n",
    "\n",
    "    (lat,lon,zval,\n",
    "     lat_deep,lon_deep,zval_deep) = get_land_sea_multipoints(sieve_polygons,sampling)\n",
    "\n",
    "\n",
    "    # sample the land/marine points onto the points within the COB Terranes\n",
    "    # This will fill the gaps that exist within continents, and average out overlaps\n",
    "\n",
    "    d,l = sampleOnSphere(pg_point_array[:,0],pg_point_array[:,1],pg_point_array[:,2],\n",
    "                         np.array(lat),np.array(lon),n=1)\n",
    "\n",
    "    land_marine_interp_points = pg_point_array[:,2].ravel()[l]\n",
    "\n",
    "\n",
    "    \n",
    "    # Deal with the mountains\n",
    "    if np.equal(reconstruction_time,time_stage_min):\n",
    "        print 'Temporary hack for valid time'\n",
    "        #dat3 = add_reconstructed_points_to_xyz(mountains_going_up_file,rotation_model,reconstruction_time,3)\n",
    "        dat4 = add_reconstructed_points_to_xyz(mountains_going_down_file,rotation_model,reconstruction_time+0.01,3)\n",
    "        dat5 = add_reconstructed_points_to_xyz(mountains_stable_file,rotation_model,reconstruction_time+0.01,3)\n",
    "        mountains_tr_point_array = np.vstack((dat4,dat5))\n",
    "        \n",
    "        dist_tr = get_distance_to_mountain_edge(mountains_tr_point_array,reconstruction_basedir,reconstruction_time)\n",
    "        dist_tr_cap = np.array(dist_tr)\n",
    "        dist_tr_cap[np.array(dist_tr)>1.] = 1.\n",
    "\n",
    "        normalized_mountain_elevation = dist_tr_cap\n",
    "        \n",
    "    else:\n",
    "        # load in the mountain points but at three different times: t1 and t2, and the reconstruction time\n",
    "        # note that these three arrays should all be identical in size, since they are the same multipoints\n",
    "        # just reconstructed to three slightly different times\n",
    "        dat3 = add_reconstructed_points_to_xyz(mountains_going_up_file,rotation_model,time_stage_max,3)\n",
    "        dat4 = add_reconstructed_points_to_xyz(mountains_going_down_file,rotation_model,time_stage_max,3)\n",
    "        dat5 = add_reconstructed_points_to_xyz(mountains_stable_file,rotation_model,time_stage_max,3)\n",
    "        mountains_t2_point_array = np.vstack((dat3,dat4,dat5))\n",
    "\n",
    "        dat3 = add_reconstructed_points_to_xyz(mountains_going_up_file,rotation_model,time_stage_min+0.01,3)\n",
    "        dat4 = add_reconstructed_points_to_xyz(mountains_going_down_file,rotation_model,time_stage_min+0.01,3)\n",
    "        dat5 = add_reconstructed_points_to_xyz(mountains_stable_file,rotation_model,time_stage_min+0.01,3)\n",
    "        mountains_t1_point_array = np.vstack((dat3,dat4,dat5))\n",
    "\n",
    "        dat3 = add_reconstructed_points_to_xyz(mountains_going_up_file,rotation_model,reconstruction_time,3)\n",
    "        dat4 = add_reconstructed_points_to_xyz(mountains_going_down_file,rotation_model,reconstruction_time,3)\n",
    "        dat5 = add_reconstructed_points_to_xyz(mountains_stable_file,rotation_model,reconstruction_time,3)\n",
    "        mountains_tr_point_array = np.vstack((dat3,dat4,dat5))\n",
    "\n",
    "        # calculate distances of the mountain points to the edge of the mountain region at t1 and t2,\n",
    "        # using the pg polygons that they should exactly correspond to \n",
    "        dist_t1 = get_distance_to_mountain_edge(mountains_t1_point_array,reconstruction_basedir,time_stage_min)\n",
    "        dist_t2 = get_distance_to_mountain_edge(mountains_t2_point_array,reconstruction_basedir,time_stage_max)\n",
    "\n",
    "\n",
    "        # cap the distances at some arbitrary value\n",
    "        dist_t1_cap = np.array(dist_t1)\n",
    "        dist_t1_cap[np.array(dist_t1)>1.] = 1.\n",
    "\n",
    "        dist_t2_cap = np.array(dist_t2)\n",
    "        dist_t2_cap[np.array(dist_t2)>1.] = 1.\n",
    "\n",
    "        # get the normalised time within this time stage\n",
    "        # for example we are at 0.25 between the t1 and t2\n",
    "        t_diff = (time_stage_max-time_stage_min)\n",
    "        t_norm = (reconstruction_time-time_stage_min)/t_diff\n",
    "        print t_diff, t_norm\n",
    "\n",
    "        # use 1d interpolation to get the 'normalized' height of the mountains at the preceding\n",
    "        # and subsequent times to the specific reconstruction time\n",
    "        # [note this is not spatial interpolation - rather it is interpolation at each individual point\n",
    "        # between the heights at earlier and later times]\n",
    "        tmp = np.vstack((dist_t1_cap,dist_t2_cap))\n",
    "        f = interpolate.interp1d([0,1],tmp.T)\n",
    "        normalized_mountain_elevation = f(t_norm)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(25,11))\n",
    "    plt.plot(mountains_tr_point_array[:,1],mountains_tr_point_array[:,0],'.')\n",
    "    \n",
    "    # interpolate the elevations at tr onto the regular long lat points that we will ultimately use \n",
    "    # for the grid output\n",
    "    # note the k value here controls number of neighbouring points used in inverse distance average\n",
    "    d,l = sampleOnSphere(mountains_tr_point_array[:,0],mountains_tr_point_array[:,1],normalized_mountain_elevation,\n",
    "                         np.array(lat),np.array(lon),k=4)\n",
    "    w = 1./d**2\n",
    "    normalized_mountain_elevation_interp_points = np.sum(w * normalized_mountain_elevation.ravel()[l],axis=1) / np.sum(w,axis=1)\n",
    "\n",
    "    mountain_buffer_distance_degrees = 1.\n",
    "    mountain_proximity_index = np.degrees(np.min(d,axis=1))<mountain_buffer_distance_degrees\n",
    "\n",
    "    plt.figure(figsize=(25,11))\n",
    "    plt.scatter(mountains_tr_point_array[:,1],mountains_tr_point_array[:,0],\n",
    "                c=normalized_mountain_elevation,edgecolor='')\n",
    "\n",
    "    write_xyz_file('land_marine.xyz',zip(lon+lon_deep,\n",
    "                                         lat+lat_deep,\n",
    "                                         np.hstack((land_marine_interp_points,zval_deep))))\n",
    "\n",
    "    mountain_elevation_factor = max_mountain_elevation/mountain_buffer_distance_degrees\n",
    "    write_xyz_file('mountain.xyz',zip(np.array(lon)[mountain_proximity_index],\n",
    "                                      np.array(lat)[mountain_proximity_index],\n",
    "                                      normalized_mountain_elevation_interp_points[mountain_proximity_index]*mountain_elevation_factor))\n",
    "\n",
    "\n",
    "    os.system('gmt xyz2grd land_marine.xyz -Gland_marine.nc -Rd -I%0.8f' % sampling)\n",
    "    os.system('gmt xyz2grd mountain.xyz -Gmountain.nc -Rd -I%0.8f -di0' % sampling)\n",
    "    os.system('gmt grdmath mountain.nc land_marine.nc ADD = paleotopo.nc')\n",
    "\n",
    "    topoX,topoY,topoZ = pg.load_netcdf('paleotopo.nc')\n",
    "\n",
    "\n",
    "    # load age grid for this time and calculate paleobathymetry\n",
    "    agegrid_file = agegrid_file_template % reconstruction_time\n",
    "\n",
    "    ageX,ageY,ageZ = pg.load_netcdf(agegrid_file)\n",
    "\n",
    "    paleodepth = pg.age2depth(ageZ,model='GDH1')\n",
    "\n",
    "\n",
    "    # get index for grid nodes where age grid is nan, replace values with topography/shallow bathymetry\n",
    "    not_bathy_index = np.isnan(paleodepth)\n",
    "\n",
    "    paleodepth[not_bathy_index] = topoZ[not_bathy_index]\n",
    "\n",
    "\n",
    "    foo = xr.DataArray(paleodepth,\n",
    "                       coords=[('lat',topoY),('lon',topoX)])\n",
    "    #foo.name = 'z'\n",
    "    foo.to_netcdf('paleotopobathy.nc',format='NETCDF3_CLASSIC')\n",
    "\n",
    "    #pg.smooth_topography_grid('paleotopobathy.nc','paleotopobathy_smooth_%0.2fMa.nc' % reconstruction_time,400.)\n",
    "    os.system('gmt grdfilter %s -G%s -Fg%0.2f -fg -D4 -Vl' % ('paleotopobathy.nc',\n",
    "                                                             'paleotopobathy_smooth.nc',\n",
    "                                                             400.))\n",
    "    os.system('gmt grdconvert %s -G%s=cf' % ('paleotopobathy_smooth.nc',\n",
    "                                             '%s/paleotopobathy_smooth_%0.2fMa.nc' % (output_dir,reconstruction_time)))\n",
    "\n",
    "    #topo_smoothX,topo_smoothY,topo_smoothZ = pg.load_netcdf('paleotopobathy_smooth.nc')\n",
    "    #\n",
    "    #plt.figure(figsize=(25,11))\n",
    "    #plt.imshow(topo_smoothZ,origin='lower',cmap=plt.cm.terrain)\n",
    "    #plt.colorbar()\n",
    "\n",
    "    #break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
